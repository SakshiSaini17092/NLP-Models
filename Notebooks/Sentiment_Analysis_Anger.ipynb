{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Anger ",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3I2m1LPsX-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b635252-beb9-412e-a2b2-d3b4350b8ac7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import string\n",
        "import nltk \n",
        "from wordsegment import load, segment\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import re \n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MeudXW3SFLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14c88cd-bcfd-45bc-c6a7-271b585c4ae4"
      },
      "source": [
        "! pip install wordsegment\n",
        "! pip install vaderSentiment\n",
        "! pip install Afinn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 8.4MB/s \n",
            "\u001b[?25hInstalling collected packages: wordsegment\n",
            "Successfully installed wordsegment-1.3.1\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting Afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Afinn\n",
            "  Building wheel for Afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Afinn: filename=afinn-0.1-cp36-none-any.whl size=53453 sha256=0429ad280f1d21c700c4d327b5ce11fde3e6e6badbf4637fada0fcd2b5b751d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built Afinn\n",
            "Installing collected packages: Afinn\n",
            "Successfully installed Afinn-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAeCv9tF0_Ub"
      },
      "source": [
        "**Fetature Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLqGcnR1KBIJ"
      },
      "source": [
        "# VADER Sentiment\n",
        "def feature_extractor_vedar(sentences):\n",
        "    intensity_anly = SentimentIntensityAnalyzer()\n",
        "\n",
        "    features = [] \n",
        "    for sent in sentences:  \n",
        "        sentiment_dict = intensity_anly.polarity_scores(sent)\n",
        "        pos = sentiment_dict['pos']\n",
        "        neg = sentiment_dict['neg']\n",
        "        neu = sentiment_dict['neu']\n",
        "        comp = sentiment_dict['compound']\n",
        "        features.append([pos, neg, neu, comp])\n",
        "\n",
        "    return np.array(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxHYqh5J4KbE"
      },
      "source": [
        "# senti word net features\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "def sentiwordnet(sentences):\n",
        "    features_sent = []\n",
        "    for sent in sentences:\n",
        "        tagged_sentence = pos_tag(word_tokenize(sent))\n",
        "        pos_score = neg_score = 0\n",
        "        for word, tag in tagged_sentence:\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "            if not lemma:\n",
        "                continue\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        "\n",
        "            synset = synsets[0]\n",
        "            swn_synset = swn.senti_synset(synset.name())\n",
        "            pos_score += swn_synset.pos_score()\n",
        "            neg_score += swn_synset.neg_score()\n",
        "        features_sent.append([pos_score, neg_score])\n",
        "    return np.array(features_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g3aLFGduqk8"
      },
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "def affin_features(sentences):\n",
        "    af = Afinn()\n",
        "    feature_affin = []\n",
        "    for sentence in sentences:\n",
        "        score = af.score(sentence)\n",
        "        feature_affin.append([score])\n",
        "    \n",
        "    return np.array(feature_affin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Pitgrey-BX"
      },
      "source": [
        "def mpqalexicon(sentences):\n",
        "\n",
        "    f = open('/content/lexicons/lexicons/2. mpqa.txt')\n",
        "    text = f.read().split('\\n')\n",
        "    dict_mpqalexicon = {}\n",
        "\n",
        "    for word in text:\n",
        "        spl = word.split('\\t')\n",
        "        if ( len(spl) == 2 ):\n",
        "            dict_mpqalexicon[spl[0]] = spl[1]\n",
        "\n",
        "    mpqla_features = []\n",
        "    for sent in sentences:\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in dict_mpqalexicon):\n",
        "                if ( dict_mpqalexicon[word] == 'negative' ):\n",
        "                    neg_score += 1\n",
        "                else:\n",
        "                    pos_score += 1\n",
        "        mpqla_features.append([pos_score, neg_score])\n",
        "\n",
        "    return np.array(mpqla_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCorKoyyL7Op"
      },
      "source": [
        "def bingliu_lexicon(sentences):\n",
        "\n",
        "    df = pd.read_csv('/content/lexicons/lexicons/1. BingLiu.csv')\n",
        "    data = df.to_numpy()\n",
        "    bing_liu_dict = {}\n",
        "\n",
        "    for text in data:\n",
        "        spl = text[0].split('\\t')\n",
        "        if ( len(spl) == 2 ):\n",
        "            bing_liu_dict[spl[0]] = spl[1]\n",
        "\n",
        "    bing_liu_features = []\n",
        "    for sent in sentences:\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in bing_liu_dict):\n",
        "                if ( bing_liu_dict[word] == 'negative' ):\n",
        "                    neg_score += 1\n",
        "                else:\n",
        "                    pos_score += 1\n",
        "        bing_liu_features.append([pos_score, neg_score])\n",
        "\n",
        "    return np.array(bing_liu_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ5-CfXVMs0B"
      },
      "source": [
        "def sentiment_140_lexicon(sentences):\n",
        "\n",
        "    f = open('/content/lexicons/lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt')\n",
        "    data = f.read().split('\\n')\n",
        "\n",
        "    senti_dict = {}\n",
        "    for text in data:\n",
        "        spl = text.split('\\t')\n",
        "        if ( len(spl) == 4 ):\n",
        "            word, pos, neg, neu = spl\n",
        "            senti_dict[word] = [pos, neg, neu]\n",
        "\n",
        "    senti_features = []\n",
        "    for sent in sentences:\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        neu_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in senti_dict):\n",
        "                pos_score += float (senti_dict[word][0])\n",
        "                neg_score += float (senti_dict[word][1])\n",
        "                neu_score += float (senti_dict[word][2])\n",
        "        \n",
        "        senti_features.append([pos_score, neg_score, neu_score])\n",
        "        \n",
        "    return np.array(senti_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bVzhLmdOOlu"
      },
      "source": [
        "def nrc_hastag_senti(sentences):\n",
        "\n",
        "    f = open('/content/lexicons/lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt')\n",
        "    data = f.read().split('\\n')\n",
        "    \n",
        "    nrc_hashtag_dict = {}\n",
        "    for text in data:\n",
        "        spl = text.split('\\t')\n",
        "        if ( len(spl) == 4 ):\n",
        "            word, score, pos, neg = spl\n",
        "            nrc_hashtag_dict[word] = [score, pos, neg]\n",
        "    \n",
        "    nrc_hashtag_features = []\n",
        "    for sent in sentences:\n",
        "        total_score = 0\n",
        "        pos_score = 0\n",
        "        neg_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in nrc_hashtag_dict):\n",
        "                total_score += float (nrc_hashtag_dict[word][0])\n",
        "                pos_score += float (nrc_hashtag_dict[word][1])\n",
        "                neg_score += float (nrc_hashtag_dict[word][2])\n",
        "        \n",
        "        nrc_hashtag_features.append([total_score, pos_score, neg_score])\n",
        "        \n",
        "    return np.array(nrc_hashtag_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjjrVnJrQ_wA"
      },
      "source": [
        "def nrc_word_emotion_lexicon(sentences):\n",
        "\n",
        "    f = open('/content/lexicons/lexicons/8. NRC-word-emotion-lexicon.txt')\n",
        "    data = f.read().split('\\n')\n",
        "\n",
        "    # print(data[:100])\n",
        "    nrc_word_emotion_dict = {}\n",
        "    for word in data:\n",
        "        spl = word.split('\\t')\n",
        "        if ( len(spl) == 3 ):\n",
        "            word, emotion, score = spl\n",
        "            # nrc_word_emotion_dict[word] = {}\n",
        "            # print(emotion)\n",
        "            if (emotion == 'anger'):\n",
        "                nrc_word_emotion_dict[word] = score\n",
        "\n",
        "    nrc_word_emotion_features = []\n",
        "    for sent in sentences:\n",
        "        total_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in nrc_word_emotion_dict):\n",
        "                total_score += float (nrc_word_emotion_dict[word])\n",
        "        \n",
        "        nrc_word_emotion_features.append([total_score])\n",
        "        \n",
        "    return np.array(nrc_word_emotion_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r51zm0ODoX31"
      },
      "source": [
        "def nrc_10_expanded(sentences):\n",
        "    df = pd.read_csv('/content/lexicons/lexicons/6. NRC-10-expanded.csv', error_bad_lines=False)\n",
        "    text = df.to_numpy()\n",
        "\n",
        "    nrc_10_expanded_dict = {}\n",
        "\n",
        "    for data in text:\n",
        "        sample = data[0]\n",
        "        spl = sample.split('\\t')\n",
        "        \n",
        "        if (len(spl) == 11):\n",
        "            word = spl[0]\n",
        "            score = spl[5]\n",
        "            nrc_10_expanded_dict[word] = score \n",
        "\n",
        "    nrc_10_expanded_features = []\n",
        "    for sent in sentences:\n",
        "        total_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in nrc_10_expanded_dict):\n",
        "                total_score += float (nrc_10_expanded_dict[word])\n",
        "        \n",
        "        nrc_10_expanded_features.append([total_score])\n",
        "        \n",
        "    return np.array(nrc_10_expanded_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mLkWTd1oXxI"
      },
      "source": [
        "def nrc_hashtag_emotion_lexicon(sentences):\n",
        "    f = open('/content/lexicons/lexicons/5. NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
        "    text = f.read().split('\\n')[35:]\n",
        "    \n",
        "    nrc_hashtag_emotion_lexicon_dict = {}\n",
        "    for data in text:\n",
        "        spl = data.split('\\t')\n",
        "        if (len(spl) == 3):\n",
        "            if (spl[0] == 'anger'):\n",
        "                nrc_hashtag_emotion_lexicon_dict[spl[1]] = spl[2]\n",
        "\n",
        "    \n",
        "    nrc_hashtag_emotion_lexicon_features = []\n",
        "    for sent in sentences:\n",
        "        total_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in nrc_hashtag_emotion_lexicon_dict):\n",
        "                total_score += float (nrc_hashtag_emotion_lexicon_dict[word])\n",
        "        \n",
        "        nrc_hashtag_emotion_lexicon_features.append([total_score])\n",
        "        \n",
        "    return np.array(nrc_hashtag_emotion_lexicon_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9MKiaeqq1YT"
      },
      "source": [
        "def afinn_emoticon(sentences):\n",
        "    f = open('/content/lexicons/lexicons/9. AFINN-emoticon-8.txt')\n",
        "    text = f.read().split('\\n')\n",
        "\n",
        "    affin_emoticon_dict = {}\n",
        "    for data in text:\n",
        "        spl = data.split('\\t')\n",
        "        if (len(spl) == 2):\n",
        "            affin_emoticon_dict[spl[0]] = spl[1]\n",
        "\n",
        "    affin_emoticon_features = []\n",
        "    for sent in sentences:\n",
        "        total_score = 0\n",
        "        for word in sent.split(' '):\n",
        "            if (word in affin_emoticon_dict):\n",
        "                total_score += float (affin_emoticon_dict[word])\n",
        "        \n",
        "        affin_emoticon_features.append([total_score])\n",
        "        \n",
        "    return np.array(affin_emoticon_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXocMhFSzgkT"
      },
      "source": [
        "# negation_features_ex(tokenised_data)\n",
        "def negation_features_ex(tokenised_data):\n",
        "\n",
        "    keywordSet = {\"don't\",\"never\", \"nothing\", \"nowhere\", \"noone\", \"none\", \"not\",\n",
        "                \"hasn't\",\"hadn't\",\"can't\",\"couldn't\",\"shouldn't\",\"won't\",\n",
        "                \"wouldn't\",\"don't\",\"doesn't\",\"didn't\",\"isn't\",\"aren't\",\"ain't\"}\n",
        "\n",
        "    negation_features = []\n",
        "    for sentence in tokenised_data:\n",
        "        punct = re.findall(r'[.:;!?]',sentence)\n",
        "        if (len(punct) > 0):\n",
        "            punct = punct[0]\n",
        "            wordSet = { x for x in re.split(\"[.:;!?, ]\",sentence) if x }\n",
        "            \n",
        "            neg_words = wordSet & keywordSet\n",
        "            tagged_sent = sentence\n",
        "            for word in neg_words:\n",
        "                start_to_w = sentence[:sentence.find(word)+len(word)]\n",
        "                w_to_punct =  re.sub(r'\\b([A-Za-z\\']+)\\b',r'\\1_NEG',\n",
        "                                sentence[sentence.find(word)+len(word):sentence.find(punct)])\n",
        "                punct_to_end = sentence[sentence.find(punct):]\n",
        "                tagged_sent = start_to_w + w_to_punct + punct_to_end\n",
        "            negation_features.append(tagged_sent)\n",
        "        else:\n",
        "            negation_features.append(sentence)\n",
        "\n",
        "\n",
        "    negation_feature_count = []\n",
        "\n",
        "    for neg_feat in negation_features:\n",
        "        count = neg_feat.count('_NEG')\n",
        "        negation_feature_count.append([count])\n",
        "\n",
        "    return negation_features, np.array(negation_feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufa7H3Y3sd8K"
      },
      "source": [
        "def to_sentences(filename):\n",
        "    # filename = \"joy-ratings-0to1train.txt\"\n",
        "    f = open(filename)\n",
        "    data = f.read().split('\\n')\n",
        "\n",
        "    score_arr = []\n",
        "    tweet_arr = \"\"\n",
        "\n",
        "    for text in data:\n",
        "        spl = text.split('\\t')\n",
        "        if ( len(spl) > 2 ):\n",
        "            score = spl[-1]\n",
        "            tweet = spl[-3]\n",
        "\n",
        "            tweet_arr += (tweet + '\\n')\n",
        "            score_arr.append(score)\n",
        "\n",
        "    f = open('text_' + filename, 'w')\n",
        "    f.write(tweet_arr)\n",
        "    return score_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K8tB0LwvdP2"
      },
      "source": [
        "def pos_tagged_tokenised(filename):\n",
        "    f = open(filename, 'r')\n",
        "    text = f.read().split('\\n')\n",
        "    tokenised_data = []\n",
        "    for data in text:\n",
        "        spl = data.split('\\t')\n",
        "        if ( len(spl) == 4):\n",
        "            tokenised_data.append(spl[0])\n",
        "    return np.array(tokenised_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4yk2hORinYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf1d6c6-9ee6-4c46-cc4a-db637f7df398"
      },
      "source": [
        "train_score = to_sentences('anger_train.txt')\n",
        "! java -Xmx500m -jar ark-tweet-nlp-0.3.2.jar text_anger_train.txt > text_anger_train_pos_tagged.txt\n",
        "\n",
        "test_score = to_sentences('anger_test.txt')\n",
        "! java -Xmx500m -jar ark-tweet-nlp-0.3.2.jar text_anger_test.txt > anger_test_pos_tagged.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected text input format\n",
            "Tokenized and tagged 857 tweets (14774 tokens) in 3.0 seconds: 286.0 tweets/sec, 4929.6 tokens/sec\n",
            "Detected text input format\n",
            "Tokenized and tagged 760 tweets (13677 tokens) in 2.8 seconds: 273.9 tweets/sec, 4928.6 tokens/sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls-6NghMjlM3"
      },
      "source": [
        "train_tokenised_data = pos_tagged_tokenised('text_anger_train_pos_tagged.txt')\n",
        "test_tokeinsed_data = pos_tagged_tokenised('anger_test_pos_tagged.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QlSFE9qy-Ff"
      },
      "source": [
        "import zipfile\n",
        "filename = \"lexicons.zip\"\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R2k-qtiq1Rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a8deec-e6c7-4889-b51d-8875c8817b6f"
      },
      "source": [
        "train_score = np.array(train_score, 'float')\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_tokenised_data, train_score, test_size=0.2, random_state=42)\n",
        "print(len(X_train), len(y_train), len(X_test), len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "685 685 172 172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B2267dJTSGm"
      },
      "source": [
        "def get_concatenated_features(tokeinsed_data):\n",
        "    # 2. Vedar features\n",
        "    vedar = feature_extractor_vedar(tokeinsed_data)\n",
        "    # 3. Lexicon based Features:\n",
        "        # a Polar word count:\n",
        "            # mpqa subjective lexicon\n",
        "    mpqla_features = mpqalexicon(tokeinsed_data)\n",
        "    # bing liu lexicon\n",
        "    bingliu_features = bingliu_lexicon(tokeinsed_data)\n",
        "    # b. Aggregate polarity scores:\n",
        "    # Sentiment140\n",
        "    sentiment_140_features = sentiment_140_lexicon(tokeinsed_data)\n",
        "    # affin\n",
        "    affin_feat = affin_features(tokeinsed_data)\n",
        "    # senti word net\n",
        "    senti_word_net_features = sentiwordnet(tokeinsed_data)\n",
        "\n",
        "    # Aggregate polarity scores (Hashtags):\n",
        "    # NRC Hashtag Sentiment lexicon\n",
        "    nrc_hashtag_sentiment_features = nrc_hastag_senti(tokeinsed_data)\n",
        "    # Emotion word count:\n",
        "    # NRC Word-Emotion Association Lexicon\n",
        "    nrc_word_emotion_features = nrc_word_emotion_lexicon(tokeinsed_data)\n",
        "\n",
        "    # Aggregate emotion score: \n",
        "    # NRC-10 Expanded lexicon\n",
        "    nrc_10_expanded_features = nrc_10_expanded(tokeinsed_data)\n",
        "\n",
        "    # Aggregate emotion score (Hashtags):\n",
        "    # NRC Hashtag Emotion Association Lexicon\n",
        "    nrc_hashtag_emotion_features = nrc_hashtag_emotion_lexicon(tokeinsed_data)\n",
        "\n",
        "    # Emoticons score\n",
        "    # AFINN emoticon\n",
        "    affin_emotion_features = afinn_emoticon(tokeinsed_data)\n",
        "\n",
        "    ## Negated fetaures\n",
        "    text_data, count_data = negation_features_ex(tokeinsed_data)\n",
        "    # concat = np.concatenate((ngram_features, ngram_features_bi, vedar, mpqla_features, \n",
        "    #                      bingliu_features, sentiment_140_features, affin_feat, \n",
        "    #                      senti_word_net_features, nrc_hashtag_sentiment_features,\n",
        "    #                      nrc_word_emotion_features, nrc_10_expanded_features,\n",
        "    #                      nrc_hashtag_emotion_features, affin_emotion_features), axis=1)\n",
        "\n",
        "    concat = np.concatenate((vedar, mpqla_features,bingliu_features,\n",
        "                             affin_feat, nrc_word_emotion_features,\n",
        "                             nrc_10_expanded_features, affin_emotion_features), axis=1)\n",
        "    return concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tZD56rEpfYd"
      },
      "source": [
        "**Training bi-grms and uni-grms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRpFBP8MpcTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047bb75b-d06a-4a63-fd1e-cd60c9080aab"
      },
      "source": [
        "rest_features = get_concatenated_features(X_train)\n",
        "cv = CountVectorizer(analyzer='word', ngram_range=(1,1), \n",
        "                        stop_words = nltk.corpus.stopwords.words('english'))\n",
        "# 1. uni ngram features\n",
        "ngram_features = cv.fit_transform(X_train).toarray()\n",
        "\n",
        "cv2 = CountVectorizer(analyzer='word', ngram_range=(2,2), \n",
        "                    stop_words = nltk.corpus.stopwords.words('english'))\n",
        "# 1. bi ngram features\n",
        "ngram_features_bi = cv2.fit_transform(X_train).toarray()\n",
        "\n",
        "train_features = np.concatenate((ngram_features, ngram_features_bi, rest_features), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 42: expected 1 fields, saw 2\\nSkipping line 49: expected 1 fields, saw 2\\nSkipping line 59: expected 1 fields, saw 2\\nSkipping line 69: expected 1 fields, saw 2\\nSkipping line 3301: expected 1 fields, saw 2\\nSkipping line 3400: expected 1 fields, saw 2\\nSkipping line 3401: expected 1 fields, saw 2\\nSkipping line 3402: expected 1 fields, saw 3\\nSkipping line 3403: expected 1 fields, saw 4\\nSkipping line 3404: expected 1 fields, saw 5\\nSkipping line 3405: expected 1 fields, saw 6\\nSkipping line 3406: expected 1 fields, saw 7\\nSkipping line 3407: expected 1 fields, saw 2\\nSkipping line 3408: expected 1 fields, saw 2\\nSkipping line 3409: expected 1 fields, saw 2\\nSkipping line 3410: expected 1 fields, saw 2\\nSkipping line 3411: expected 1 fields, saw 2\\nSkipping line 3412: expected 1 fields, saw 2\\nSkipping line 3422: expected 1 fields, saw 2\\nSkipping line 3637: expected 1 fields, saw 2\\nSkipping line 3638: expected 1 fields, saw 3\\nSkipping line 3639: expected 1 fields, saw 2\\nSkipping line 3640: expected 1 fields, saw 2\\nSkipping line 3646: expected 1 fields, saw 2\\nSkipping line 3653: expected 1 fields, saw 2\\nSkipping line 3659: expected 1 fields, saw 2\\nSkipping line 3856: expected 1 fields, saw 2\\nSkipping line 3857: expected 1 fields, saw 3\\nSkipping line 3858: expected 1 fields, saw 2\\nSkipping line 3859: expected 1 fields, saw 2\\nSkipping line 3860: expected 1 fields, saw 2\\nSkipping line 3861: expected 1 fields, saw 2\\nSkipping line 3862: expected 1 fields, saw 2\\nSkipping line 3863: expected 1 fields, saw 2\\nSkipping line 3864: expected 1 fields, saw 2\\nSkipping line 3865: expected 1 fields, saw 2\\nSkipping line 3866: expected 1 fields, saw 2\\nSkipping line 3867: expected 1 fields, saw 2\\nSkipping line 3924: expected 1 fields, saw 2\\nSkipping line 3963: expected 1 fields, saw 2\\nSkipping line 4054: expected 1 fields, saw 2\\nSkipping line 4153: expected 1 fields, saw 2\\nSkipping line 4193: expected 1 fields, saw 2\\nSkipping line 4244: expected 1 fields, saw 2\\nSkipping line 4294: expected 1 fields, saw 2\\nSkipping line 4310: expected 1 fields, saw 2\\nSkipping line 4330: expected 1 fields, saw 2\\nSkipping line 4364: expected 1 fields, saw 2\\nSkipping line 4378: expected 1 fields, saw 2\\nSkipping line 4405: expected 1 fields, saw 2\\nSkipping line 4418: expected 1 fields, saw 2\\nSkipping line 4440: expected 1 fields, saw 2\\nSkipping line 4468: expected 1 fields, saw 2\\nSkipping line 4667: expected 1 fields, saw 2\\nSkipping line 4668: expected 1 fields, saw 2\\nSkipping line 4669: expected 1 fields, saw 2\\nSkipping line 4670: expected 1 fields, saw 2\\nSkipping line 4671: expected 1 fields, saw 2\\nSkipping line 4672: expected 1 fields, saw 2\\nSkipping line 4673: expected 1 fields, saw 2\\nSkipping line 4717: expected 1 fields, saw 2\\nSkipping line 4732: expected 1 fields, saw 2\\nSkipping line 4846: expected 1 fields, saw 2\\nSkipping line 4890: expected 1 fields, saw 2\\nSkipping line 4918: expected 1 fields, saw 2\\nSkipping line 4924: expected 1 fields, saw 2\\nSkipping line 4938: expected 1 fields, saw 2\\nSkipping line 4963: expected 1 fields, saw 2\\nSkipping line 5099: expected 1 fields, saw 2\\nSkipping line 5100: expected 1 fields, saw 2\\nSkipping line 5101: expected 1 fields, saw 2\\nSkipping line 5142: expected 1 fields, saw 2\\nSkipping line 5150: expected 1 fields, saw 2\\nSkipping line 5211: expected 1 fields, saw 2\\nSkipping line 5221: expected 1 fields, saw 2\\nSkipping line 5224: expected 1 fields, saw 2\\nSkipping line 5319: expected 1 fields, saw 2\\nSkipping line 5320: expected 1 fields, saw 2\\nSkipping line 5358: expected 1 fields, saw 2\\nSkipping line 5364: expected 1 fields, saw 2\\nSkipping line 5415: expected 1 fields, saw 2\\nSkipping line 5508: expected 1 fields, saw 2\\nSkipping line 5548: expected 1 fields, saw 2\\nSkipping line 5552: expected 1 fields, saw 2\\nSkipping line 5599: expected 1 fields, saw 2\\nSkipping line 5662: expected 1 fields, saw 2\\nSkipping line 5663: expected 1 fields, saw 2\\nSkipping line 5696: expected 1 fields, saw 2\\nSkipping line 5699: expected 1 fields, saw 2\\nSkipping line 5731: expected 1 fields, saw 2\\nSkipping line 5773: expected 1 fields, saw 2\\nSkipping line 5809: expected 1 fields, saw 2\\nSkipping line 5843: expected 1 fields, saw 2\\nSkipping line 5889: expected 1 fields, saw 2\\nSkipping line 5890: expected 1 fields, saw 2\\nSkipping line 5924: expected 1 fields, saw 2\\nSkipping line 5963: expected 1 fields, saw 2\\nSkipping line 6012: expected 1 fields, saw 2\\nSkipping line 6041: expected 1 fields, saw 2\\nSkipping line 6143: expected 1 fields, saw 2\\nSkipping line 6326: expected 1 fields, saw 2\\nSkipping line 6380: expected 1 fields, saw 2\\nSkipping line 6395: expected 1 fields, saw 2\\nSkipping line 11798: expected 1 fields, saw 2\\nSkipping line 15316: expected 1 fields, saw 2\\nSkipping line 24217: expected 1 fields, saw 2\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5KxYnkaqQRK"
      },
      "source": [
        "**Testing data bi-grms and uni-grms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LhHEF0YpppZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfae146-aa2f-4ef0-a45e-88ca6c4c4267"
      },
      "source": [
        "rest_features = get_concatenated_features(X_test)\n",
        "# 1. uni ngram features\n",
        "ngram_features = cv.transform(X_test).toarray()\n",
        "# 1. bi ngram features\n",
        "ngram_features_bi = cv2.transform(X_test).toarray()\n",
        "test_features = np.concatenate((ngram_features, ngram_features_bi, rest_features), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 42: expected 1 fields, saw 2\\nSkipping line 49: expected 1 fields, saw 2\\nSkipping line 59: expected 1 fields, saw 2\\nSkipping line 69: expected 1 fields, saw 2\\nSkipping line 3301: expected 1 fields, saw 2\\nSkipping line 3400: expected 1 fields, saw 2\\nSkipping line 3401: expected 1 fields, saw 2\\nSkipping line 3402: expected 1 fields, saw 3\\nSkipping line 3403: expected 1 fields, saw 4\\nSkipping line 3404: expected 1 fields, saw 5\\nSkipping line 3405: expected 1 fields, saw 6\\nSkipping line 3406: expected 1 fields, saw 7\\nSkipping line 3407: expected 1 fields, saw 2\\nSkipping line 3408: expected 1 fields, saw 2\\nSkipping line 3409: expected 1 fields, saw 2\\nSkipping line 3410: expected 1 fields, saw 2\\nSkipping line 3411: expected 1 fields, saw 2\\nSkipping line 3412: expected 1 fields, saw 2\\nSkipping line 3422: expected 1 fields, saw 2\\nSkipping line 3637: expected 1 fields, saw 2\\nSkipping line 3638: expected 1 fields, saw 3\\nSkipping line 3639: expected 1 fields, saw 2\\nSkipping line 3640: expected 1 fields, saw 2\\nSkipping line 3646: expected 1 fields, saw 2\\nSkipping line 3653: expected 1 fields, saw 2\\nSkipping line 3659: expected 1 fields, saw 2\\nSkipping line 3856: expected 1 fields, saw 2\\nSkipping line 3857: expected 1 fields, saw 3\\nSkipping line 3858: expected 1 fields, saw 2\\nSkipping line 3859: expected 1 fields, saw 2\\nSkipping line 3860: expected 1 fields, saw 2\\nSkipping line 3861: expected 1 fields, saw 2\\nSkipping line 3862: expected 1 fields, saw 2\\nSkipping line 3863: expected 1 fields, saw 2\\nSkipping line 3864: expected 1 fields, saw 2\\nSkipping line 3865: expected 1 fields, saw 2\\nSkipping line 3866: expected 1 fields, saw 2\\nSkipping line 3867: expected 1 fields, saw 2\\nSkipping line 3924: expected 1 fields, saw 2\\nSkipping line 3963: expected 1 fields, saw 2\\nSkipping line 4054: expected 1 fields, saw 2\\nSkipping line 4153: expected 1 fields, saw 2\\nSkipping line 4193: expected 1 fields, saw 2\\nSkipping line 4244: expected 1 fields, saw 2\\nSkipping line 4294: expected 1 fields, saw 2\\nSkipping line 4310: expected 1 fields, saw 2\\nSkipping line 4330: expected 1 fields, saw 2\\nSkipping line 4364: expected 1 fields, saw 2\\nSkipping line 4378: expected 1 fields, saw 2\\nSkipping line 4405: expected 1 fields, saw 2\\nSkipping line 4418: expected 1 fields, saw 2\\nSkipping line 4440: expected 1 fields, saw 2\\nSkipping line 4468: expected 1 fields, saw 2\\nSkipping line 4667: expected 1 fields, saw 2\\nSkipping line 4668: expected 1 fields, saw 2\\nSkipping line 4669: expected 1 fields, saw 2\\nSkipping line 4670: expected 1 fields, saw 2\\nSkipping line 4671: expected 1 fields, saw 2\\nSkipping line 4672: expected 1 fields, saw 2\\nSkipping line 4673: expected 1 fields, saw 2\\nSkipping line 4717: expected 1 fields, saw 2\\nSkipping line 4732: expected 1 fields, saw 2\\nSkipping line 4846: expected 1 fields, saw 2\\nSkipping line 4890: expected 1 fields, saw 2\\nSkipping line 4918: expected 1 fields, saw 2\\nSkipping line 4924: expected 1 fields, saw 2\\nSkipping line 4938: expected 1 fields, saw 2\\nSkipping line 4963: expected 1 fields, saw 2\\nSkipping line 5099: expected 1 fields, saw 2\\nSkipping line 5100: expected 1 fields, saw 2\\nSkipping line 5101: expected 1 fields, saw 2\\nSkipping line 5142: expected 1 fields, saw 2\\nSkipping line 5150: expected 1 fields, saw 2\\nSkipping line 5211: expected 1 fields, saw 2\\nSkipping line 5221: expected 1 fields, saw 2\\nSkipping line 5224: expected 1 fields, saw 2\\nSkipping line 5319: expected 1 fields, saw 2\\nSkipping line 5320: expected 1 fields, saw 2\\nSkipping line 5358: expected 1 fields, saw 2\\nSkipping line 5364: expected 1 fields, saw 2\\nSkipping line 5415: expected 1 fields, saw 2\\nSkipping line 5508: expected 1 fields, saw 2\\nSkipping line 5548: expected 1 fields, saw 2\\nSkipping line 5552: expected 1 fields, saw 2\\nSkipping line 5599: expected 1 fields, saw 2\\nSkipping line 5662: expected 1 fields, saw 2\\nSkipping line 5663: expected 1 fields, saw 2\\nSkipping line 5696: expected 1 fields, saw 2\\nSkipping line 5699: expected 1 fields, saw 2\\nSkipping line 5731: expected 1 fields, saw 2\\nSkipping line 5773: expected 1 fields, saw 2\\nSkipping line 5809: expected 1 fields, saw 2\\nSkipping line 5843: expected 1 fields, saw 2\\nSkipping line 5889: expected 1 fields, saw 2\\nSkipping line 5890: expected 1 fields, saw 2\\nSkipping line 5924: expected 1 fields, saw 2\\nSkipping line 5963: expected 1 fields, saw 2\\nSkipping line 6012: expected 1 fields, saw 2\\nSkipping line 6041: expected 1 fields, saw 2\\nSkipping line 6143: expected 1 fields, saw 2\\nSkipping line 6326: expected 1 fields, saw 2\\nSkipping line 6380: expected 1 fields, saw 2\\nSkipping line 6395: expected 1 fields, saw 2\\nSkipping line 11798: expected 1 fields, saw 2\\nSkipping line 15316: expected 1 fields, saw 2\\nSkipping line 24217: expected 1 fields, saw 2\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MIaqG_3lOPf"
      },
      "source": [
        "**SVM Regression Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLAarmSLkui4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fac875-9ad2-4e88-ecbf-17c703b1aaec"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_model = SVR(kernel='linear', \n",
        "                degree = 3, max_iter = -1, gamma = 'auto')\n",
        "svm_model.fit(train_features, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
              "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqV6l1cFkufX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0b9b9f-a646-4a44-a36c-d19598370d2c"
      },
      "source": [
        "# mpqla : 0.3542180321668468\n",
        "# bingliu : 0.3620280056526838\n",
        "# affin : 0.37217170874188976\n",
        "# nrc word emotion : 0.37249692201319107\n",
        "# nrc 10 expanded : 0.37656293113960404\n",
        "# affin 0.37688994220118466\n",
        "y_pred_svm = svm_model.predict(test_features)\n",
        "r2_score(y_test, y_pred_svm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37688994220118466"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGa3oQf1lXHj"
      },
      "source": [
        "**Decision Tree Regression Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApcEEJgClWac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7d588d-e76f-40dd-c11e-bec3fb4dc392"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "decision_tree_model = DecisionTreeRegressor(max_depth= 5, \n",
        "                                            random_state= 48)\n",
        "decision_tree_model.fit(train_features, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=48, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Q-kX88lWJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c711c324-7fc0-4f2e-92d2-892c7834de2d"
      },
      "source": [
        "y_pred_dtree = decision_tree_model.predict(test_features)\n",
        "r2_score(y_test, y_pred_dtree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22758606099567713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWeV0mXlnWo"
      },
      "source": [
        "**MLP Regression Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Wv-ivmlsmU"
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "mlpRegressor_model = MLPRegressor(random_state=1, max_iter = 350, \n",
        "                                  early_stopping = True, warm_start = True\n",
        "                                  , learning_rate_init = 0.1, solver = 'sgd', validation_fraction = 0.2)\n",
        "mlpRegressor_model.fit(train_features, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L67Vrnu7mEdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adb6595-b754-4599-bede-f9c84bc047e8"
      },
      "source": [
        "y_pred_mlp = mlpRegressor_model.predict(test_features)\n",
        "r2_score(y_test, y_pred_mlp)\n",
        "# 0.3530634846189322"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3530634846189322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHdmy3So033B"
      },
      "source": [
        "**Submission Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQBchJm1yUv8"
      },
      "source": [
        "def make_submission(filename, y_pred):\n",
        "    f = open('anger_test.txt')\n",
        "    data = f.read().split('\\n')\n",
        "    score_arr = []\n",
        "    tweet_arr = \"\"\n",
        "    a = ''\n",
        "    i = 0\n",
        "    for text in data:\n",
        "            spl = text.split('\\t')\n",
        "            if ( len(spl) == 4 ):\n",
        "                spl[-1] = str(y_pred[i])\n",
        "\n",
        "            a += '\\t'.join(spl) + '\\n'\n",
        "            i += 1\n",
        "    f = open(filename, 'w')\n",
        "    f.write(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJyb6G_rvlJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa94b25-f645-4cd1-ec16-a68efdde9a1b"
      },
      "source": [
        "rest_features = get_concatenated_features(train_tokenised_data)\n",
        "cv = CountVectorizer(analyzer='word', ngram_range=(1,1), \n",
        "                        stop_words = nltk.corpus.stopwords.words('english'))\n",
        "# 1. uni ngram features\n",
        "ngram_features = cv.fit_transform(train_tokenised_data).toarray()\n",
        "\n",
        "cv2 = CountVectorizer(analyzer='word', ngram_range=(2,2), \n",
        "                    stop_words = nltk.corpus.stopwords.words('english'))\n",
        "# 1. bi ngram features\n",
        "ngram_features_bi = cv2.fit_transform(train_tokenised_data).toarray()\n",
        "\n",
        "train_features = np.concatenate( (ngram_features, ngram_features_bi, rest_features), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 42: expected 1 fields, saw 2\\nSkipping line 49: expected 1 fields, saw 2\\nSkipping line 59: expected 1 fields, saw 2\\nSkipping line 69: expected 1 fields, saw 2\\nSkipping line 3301: expected 1 fields, saw 2\\nSkipping line 3400: expected 1 fields, saw 2\\nSkipping line 3401: expected 1 fields, saw 2\\nSkipping line 3402: expected 1 fields, saw 3\\nSkipping line 3403: expected 1 fields, saw 4\\nSkipping line 3404: expected 1 fields, saw 5\\nSkipping line 3405: expected 1 fields, saw 6\\nSkipping line 3406: expected 1 fields, saw 7\\nSkipping line 3407: expected 1 fields, saw 2\\nSkipping line 3408: expected 1 fields, saw 2\\nSkipping line 3409: expected 1 fields, saw 2\\nSkipping line 3410: expected 1 fields, saw 2\\nSkipping line 3411: expected 1 fields, saw 2\\nSkipping line 3412: expected 1 fields, saw 2\\nSkipping line 3422: expected 1 fields, saw 2\\nSkipping line 3637: expected 1 fields, saw 2\\nSkipping line 3638: expected 1 fields, saw 3\\nSkipping line 3639: expected 1 fields, saw 2\\nSkipping line 3640: expected 1 fields, saw 2\\nSkipping line 3646: expected 1 fields, saw 2\\nSkipping line 3653: expected 1 fields, saw 2\\nSkipping line 3659: expected 1 fields, saw 2\\nSkipping line 3856: expected 1 fields, saw 2\\nSkipping line 3857: expected 1 fields, saw 3\\nSkipping line 3858: expected 1 fields, saw 2\\nSkipping line 3859: expected 1 fields, saw 2\\nSkipping line 3860: expected 1 fields, saw 2\\nSkipping line 3861: expected 1 fields, saw 2\\nSkipping line 3862: expected 1 fields, saw 2\\nSkipping line 3863: expected 1 fields, saw 2\\nSkipping line 3864: expected 1 fields, saw 2\\nSkipping line 3865: expected 1 fields, saw 2\\nSkipping line 3866: expected 1 fields, saw 2\\nSkipping line 3867: expected 1 fields, saw 2\\nSkipping line 3924: expected 1 fields, saw 2\\nSkipping line 3963: expected 1 fields, saw 2\\nSkipping line 4054: expected 1 fields, saw 2\\nSkipping line 4153: expected 1 fields, saw 2\\nSkipping line 4193: expected 1 fields, saw 2\\nSkipping line 4244: expected 1 fields, saw 2\\nSkipping line 4294: expected 1 fields, saw 2\\nSkipping line 4310: expected 1 fields, saw 2\\nSkipping line 4330: expected 1 fields, saw 2\\nSkipping line 4364: expected 1 fields, saw 2\\nSkipping line 4378: expected 1 fields, saw 2\\nSkipping line 4405: expected 1 fields, saw 2\\nSkipping line 4418: expected 1 fields, saw 2\\nSkipping line 4440: expected 1 fields, saw 2\\nSkipping line 4468: expected 1 fields, saw 2\\nSkipping line 4667: expected 1 fields, saw 2\\nSkipping line 4668: expected 1 fields, saw 2\\nSkipping line 4669: expected 1 fields, saw 2\\nSkipping line 4670: expected 1 fields, saw 2\\nSkipping line 4671: expected 1 fields, saw 2\\nSkipping line 4672: expected 1 fields, saw 2\\nSkipping line 4673: expected 1 fields, saw 2\\nSkipping line 4717: expected 1 fields, saw 2\\nSkipping line 4732: expected 1 fields, saw 2\\nSkipping line 4846: expected 1 fields, saw 2\\nSkipping line 4890: expected 1 fields, saw 2\\nSkipping line 4918: expected 1 fields, saw 2\\nSkipping line 4924: expected 1 fields, saw 2\\nSkipping line 4938: expected 1 fields, saw 2\\nSkipping line 4963: expected 1 fields, saw 2\\nSkipping line 5099: expected 1 fields, saw 2\\nSkipping line 5100: expected 1 fields, saw 2\\nSkipping line 5101: expected 1 fields, saw 2\\nSkipping line 5142: expected 1 fields, saw 2\\nSkipping line 5150: expected 1 fields, saw 2\\nSkipping line 5211: expected 1 fields, saw 2\\nSkipping line 5221: expected 1 fields, saw 2\\nSkipping line 5224: expected 1 fields, saw 2\\nSkipping line 5319: expected 1 fields, saw 2\\nSkipping line 5320: expected 1 fields, saw 2\\nSkipping line 5358: expected 1 fields, saw 2\\nSkipping line 5364: expected 1 fields, saw 2\\nSkipping line 5415: expected 1 fields, saw 2\\nSkipping line 5508: expected 1 fields, saw 2\\nSkipping line 5548: expected 1 fields, saw 2\\nSkipping line 5552: expected 1 fields, saw 2\\nSkipping line 5599: expected 1 fields, saw 2\\nSkipping line 5662: expected 1 fields, saw 2\\nSkipping line 5663: expected 1 fields, saw 2\\nSkipping line 5696: expected 1 fields, saw 2\\nSkipping line 5699: expected 1 fields, saw 2\\nSkipping line 5731: expected 1 fields, saw 2\\nSkipping line 5773: expected 1 fields, saw 2\\nSkipping line 5809: expected 1 fields, saw 2\\nSkipping line 5843: expected 1 fields, saw 2\\nSkipping line 5889: expected 1 fields, saw 2\\nSkipping line 5890: expected 1 fields, saw 2\\nSkipping line 5924: expected 1 fields, saw 2\\nSkipping line 5963: expected 1 fields, saw 2\\nSkipping line 6012: expected 1 fields, saw 2\\nSkipping line 6041: expected 1 fields, saw 2\\nSkipping line 6143: expected 1 fields, saw 2\\nSkipping line 6326: expected 1 fields, saw 2\\nSkipping line 6380: expected 1 fields, saw 2\\nSkipping line 6395: expected 1 fields, saw 2\\nSkipping line 11798: expected 1 fields, saw 2\\nSkipping line 15316: expected 1 fields, saw 2\\nSkipping line 24217: expected 1 fields, saw 2\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZiIWuEAv-GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8eee2ef-0d10-4ff1-f1a0-2c9d29486fcc"
      },
      "source": [
        "rest_features = get_concatenated_features(test_tokeinsed_data)\n",
        "# 1. uni ngram features\n",
        "ngram_features = cv.transform(test_tokeinsed_data).toarray()\n",
        "# 1. bi ngram features\n",
        "ngram_features_bi = cv2.transform(test_tokeinsed_data).toarray()\n",
        "test_features = np.concatenate( (ngram_features, ngram_features_bi, rest_features), axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 42: expected 1 fields, saw 2\\nSkipping line 49: expected 1 fields, saw 2\\nSkipping line 59: expected 1 fields, saw 2\\nSkipping line 69: expected 1 fields, saw 2\\nSkipping line 3301: expected 1 fields, saw 2\\nSkipping line 3400: expected 1 fields, saw 2\\nSkipping line 3401: expected 1 fields, saw 2\\nSkipping line 3402: expected 1 fields, saw 3\\nSkipping line 3403: expected 1 fields, saw 4\\nSkipping line 3404: expected 1 fields, saw 5\\nSkipping line 3405: expected 1 fields, saw 6\\nSkipping line 3406: expected 1 fields, saw 7\\nSkipping line 3407: expected 1 fields, saw 2\\nSkipping line 3408: expected 1 fields, saw 2\\nSkipping line 3409: expected 1 fields, saw 2\\nSkipping line 3410: expected 1 fields, saw 2\\nSkipping line 3411: expected 1 fields, saw 2\\nSkipping line 3412: expected 1 fields, saw 2\\nSkipping line 3422: expected 1 fields, saw 2\\nSkipping line 3637: expected 1 fields, saw 2\\nSkipping line 3638: expected 1 fields, saw 3\\nSkipping line 3639: expected 1 fields, saw 2\\nSkipping line 3640: expected 1 fields, saw 2\\nSkipping line 3646: expected 1 fields, saw 2\\nSkipping line 3653: expected 1 fields, saw 2\\nSkipping line 3659: expected 1 fields, saw 2\\nSkipping line 3856: expected 1 fields, saw 2\\nSkipping line 3857: expected 1 fields, saw 3\\nSkipping line 3858: expected 1 fields, saw 2\\nSkipping line 3859: expected 1 fields, saw 2\\nSkipping line 3860: expected 1 fields, saw 2\\nSkipping line 3861: expected 1 fields, saw 2\\nSkipping line 3862: expected 1 fields, saw 2\\nSkipping line 3863: expected 1 fields, saw 2\\nSkipping line 3864: expected 1 fields, saw 2\\nSkipping line 3865: expected 1 fields, saw 2\\nSkipping line 3866: expected 1 fields, saw 2\\nSkipping line 3867: expected 1 fields, saw 2\\nSkipping line 3924: expected 1 fields, saw 2\\nSkipping line 3963: expected 1 fields, saw 2\\nSkipping line 4054: expected 1 fields, saw 2\\nSkipping line 4153: expected 1 fields, saw 2\\nSkipping line 4193: expected 1 fields, saw 2\\nSkipping line 4244: expected 1 fields, saw 2\\nSkipping line 4294: expected 1 fields, saw 2\\nSkipping line 4310: expected 1 fields, saw 2\\nSkipping line 4330: expected 1 fields, saw 2\\nSkipping line 4364: expected 1 fields, saw 2\\nSkipping line 4378: expected 1 fields, saw 2\\nSkipping line 4405: expected 1 fields, saw 2\\nSkipping line 4418: expected 1 fields, saw 2\\nSkipping line 4440: expected 1 fields, saw 2\\nSkipping line 4468: expected 1 fields, saw 2\\nSkipping line 4667: expected 1 fields, saw 2\\nSkipping line 4668: expected 1 fields, saw 2\\nSkipping line 4669: expected 1 fields, saw 2\\nSkipping line 4670: expected 1 fields, saw 2\\nSkipping line 4671: expected 1 fields, saw 2\\nSkipping line 4672: expected 1 fields, saw 2\\nSkipping line 4673: expected 1 fields, saw 2\\nSkipping line 4717: expected 1 fields, saw 2\\nSkipping line 4732: expected 1 fields, saw 2\\nSkipping line 4846: expected 1 fields, saw 2\\nSkipping line 4890: expected 1 fields, saw 2\\nSkipping line 4918: expected 1 fields, saw 2\\nSkipping line 4924: expected 1 fields, saw 2\\nSkipping line 4938: expected 1 fields, saw 2\\nSkipping line 4963: expected 1 fields, saw 2\\nSkipping line 5099: expected 1 fields, saw 2\\nSkipping line 5100: expected 1 fields, saw 2\\nSkipping line 5101: expected 1 fields, saw 2\\nSkipping line 5142: expected 1 fields, saw 2\\nSkipping line 5150: expected 1 fields, saw 2\\nSkipping line 5211: expected 1 fields, saw 2\\nSkipping line 5221: expected 1 fields, saw 2\\nSkipping line 5224: expected 1 fields, saw 2\\nSkipping line 5319: expected 1 fields, saw 2\\nSkipping line 5320: expected 1 fields, saw 2\\nSkipping line 5358: expected 1 fields, saw 2\\nSkipping line 5364: expected 1 fields, saw 2\\nSkipping line 5415: expected 1 fields, saw 2\\nSkipping line 5508: expected 1 fields, saw 2\\nSkipping line 5548: expected 1 fields, saw 2\\nSkipping line 5552: expected 1 fields, saw 2\\nSkipping line 5599: expected 1 fields, saw 2\\nSkipping line 5662: expected 1 fields, saw 2\\nSkipping line 5663: expected 1 fields, saw 2\\nSkipping line 5696: expected 1 fields, saw 2\\nSkipping line 5699: expected 1 fields, saw 2\\nSkipping line 5731: expected 1 fields, saw 2\\nSkipping line 5773: expected 1 fields, saw 2\\nSkipping line 5809: expected 1 fields, saw 2\\nSkipping line 5843: expected 1 fields, saw 2\\nSkipping line 5889: expected 1 fields, saw 2\\nSkipping line 5890: expected 1 fields, saw 2\\nSkipping line 5924: expected 1 fields, saw 2\\nSkipping line 5963: expected 1 fields, saw 2\\nSkipping line 6012: expected 1 fields, saw 2\\nSkipping line 6041: expected 1 fields, saw 2\\nSkipping line 6143: expected 1 fields, saw 2\\nSkipping line 6326: expected 1 fields, saw 2\\nSkipping line 6380: expected 1 fields, saw 2\\nSkipping line 6395: expected 1 fields, saw 2\\nSkipping line 11798: expected 1 fields, saw 2\\nSkipping line 15316: expected 1 fields, saw 2\\nSkipping line 24217: expected 1 fields, saw 2\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdNk5vv1vlG7"
      },
      "source": [
        "svm_model.fit(train_features, train_score)\n",
        "submission_svm_pred = svm_model.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRbAGRTQvlDK"
      },
      "source": [
        "decision_tree_model.fit(train_features, train_score)\n",
        "submission_dtree_pred = decision_tree_model.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B5tUTy8wmXH"
      },
      "source": [
        "mlpRegressor_model.fit(train_features, train_score)\n",
        "submission_mlp_pred = mlpRegressor_model.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5H-EufTVXXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c9b6ef-f0c5-47de-aff9-deb9633af5b6"
      },
      "source": [
        "! git clone https://github.com/felipebravom/EmoInt.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EmoInt'...\n",
            "remote: Enumerating objects: 264, done.\u001b[K\n",
            "remote: Total 264 (delta 0), reused 0 (delta 0), pack-reused 264\u001b[K\n",
            "Receiving objects: 100% (264/264), 1016.20 KiB | 2.00 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X5GDL1mmqN5"
      },
      "source": [
        "make_submission('svm_model_submission.txt', submission_svm_pred)\n",
        "make_submission('decision_tree_model_submission.txt', submission_dtree_pred)\n",
        "make_submission('mlp_submission.txt', submission_mlp_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD9-zMANmqKO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suP08xbZWKPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b13fe89-b341-46fc-e324-feab2112a224"
      },
      "source": [
        "! python2 /content/EmoInt/evaluate.py 1 '/content/svm_model_submission.txt' '/content/anger-pred.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson correlation between /content/svm_model_submission.txt and /content/anger-pred.txt:\t0.7852915877156502\n",
            "Spearman correlation between /content/svm_model_submission.txt and /content/anger-pred.txt:\t0.7748515715394922\n",
            "Pearson correlation for gold scores in range 0.5-1 between /content/svm_model_submission.txt and /content/anger-pred.txt:\t0.6436375012978239\n",
            "Spearman correlation for gold scores in range 0.5-1 between /content/svm_model_submission.txt and /content/anger-pred.txt:\t0.5805008849820491\n",
            "\n",
            "Average Pearson correlation:\t0.7852915877156502\n",
            "Average Spearman correlation:\t0.7748515715394922\n",
            "Average Pearson correlation for gold scores in range 0.5-1:\t0.6436375012978239\n",
            "Average Spearman correlationfor gold scores in range 0.5-1:\t0.5805008849820491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNEdj2SjGm-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dad823-ae25-4617-b473-c6fcd38c6c78"
      },
      "source": [
        "! python2 /content/EmoInt/evaluate.py 1 '/content/decision_tree_model_submission.txt' '/content/anger-pred.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson correlation between /content/decision_tree_model_submission.txt and /content/anger-pred.txt:\t0.7029358766445881\n",
            "Spearman correlation between /content/decision_tree_model_submission.txt and /content/anger-pred.txt:\t0.7314147810017549\n",
            "Pearson correlation for gold scores in range 0.5-1 between /content/decision_tree_model_submission.txt and /content/anger-pred.txt:\t0.4950080818920411\n",
            "Spearman correlation for gold scores in range 0.5-1 between /content/decision_tree_model_submission.txt and /content/anger-pred.txt:\t0.45583600381925915\n",
            "\n",
            "Average Pearson correlation:\t0.7029358766445881\n",
            "Average Spearman correlation:\t0.7314147810017549\n",
            "Average Pearson correlation for gold scores in range 0.5-1:\t0.4950080818920411\n",
            "Average Spearman correlationfor gold scores in range 0.5-1:\t0.45583600381925915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1sEDcLpyU20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9093a2cb-93e9-47ad-c8bf-466d00847f71"
      },
      "source": [
        "! python2 /content/EmoInt/evaluate.py 1 '/content/mlp_submission.txt' '/content/anger-pred.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson correlation between /content/mlp_submission.txt and /content/anger-pred.txt:\t0.7529335653765463\n",
            "Spearman correlation between /content/mlp_submission.txt and /content/anger-pred.txt:\t0.7382077278755588\n",
            "Pearson correlation for gold scores in range 0.5-1 between /content/mlp_submission.txt and /content/anger-pred.txt:\t0.660883611535093\n",
            "Spearman correlation for gold scores in range 0.5-1 between /content/mlp_submission.txt and /content/anger-pred.txt:\t0.5546694993659812\n",
            "\n",
            "Average Pearson correlation:\t0.7529335653765463\n",
            "Average Spearman correlation:\t0.7382077278755588\n",
            "Average Pearson correlation for gold scores in range 0.5-1:\t0.660883611535093\n",
            "Average Spearman correlationfor gold scores in range 0.5-1:\t0.5546694993659812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQuSgihKyUy1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2bXr9atW0kr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}